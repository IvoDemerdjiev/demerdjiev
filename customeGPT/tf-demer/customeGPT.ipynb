{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f189b687-e395-4ac0-8a46-57ec7fe7d433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, initializers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, ReLU, Dropout,LayerNormalization\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "# ignore cuda warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73f5ca20-886f-41ff-807b-d8e7f1dda496",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "max_iters = 500\n",
    "learning_rate = 1e-4\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"GPU is available\")\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    device = '/device:CPU:0'\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fab7e67d-1426-48db-b701-9c934cae5ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_extractor():\n",
    "    path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "    \n",
    "    #text = open(path_to_file, 'rb').read().decode(encoding='utf-8').replace('\\n', '\\n')\n",
    "    with open(path_to_file, 'rb') as f:\n",
    "        text = f.read().decode('utf-8')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "baf74053-50dd-4bb1-8197-d8aeb1941b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every character in text:\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size=65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "text = text_extractor()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Every character in text:\" + ''.join(chars))\n",
    "print(\"Vocab size={0}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a321d21b-cbda-4632-b93b-f4c8c09eb40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_encoder(text):\n",
    "    # encoder: take a string, output a list of integers\n",
    "    stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "    encode = lambda s: [stoi[ch] for ch in s]\n",
    "    # Encoding the entire text into numbers\n",
    "    series = encode(text)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e34309df-9be4-41d3-9689-fad61f5df712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_decoder(text):\n",
    "    # decoder: take a list of integers, output a string\n",
    "    itos = {i:ch for i, ch in enumerate(chars)} \n",
    "    decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "    series = decode(text)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7bc831c-a9a7-4ab4-be76-36db7d628775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a windowed datase\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    # Creating a tensorflow dataset from the encoded series\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    # Creating a windowed dataset with each window of size window_size + 1 and shifting the window by 1 after each step\n",
    "    dataset = dataset.window(size=window_size+1, shift = 1, drop_remainder=True)\n",
    "    # Flattening the dataset\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "    # Splitting each window into features (all elements except the last) and target (the last element)\n",
    "    dataset = dataset.map(lambda x: (x[:-1], x[1:]))\n",
    "    # Shuffling the dataset\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    # Batching the dataset and prefetching 1 batch at a time to improve performance\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bad87ad3-1514-485a-b5ff-d549f7d6c5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "series = text_encoder(text)\n",
    "n = int(0.9*len(series)) # first 90% will be train, rest val\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = windowed_dataset(series[:n], block_size, batch_size, shuffle_buffer=10)\n",
    "\n",
    "# Create the testing dataset\n",
    "test_dataset = windowed_dataset(series[n:], block_size, batch_size, shuffle_buffer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "626a6e29-6d81-4b5f-a8eb-d40153d04e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs_shape: (64,)\n",
      "decoder_inputs_shape: (64,)\n",
      "targets_shape: (32, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 12:32:24.134615: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32 and shape [1003854]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-11-07 12:32:24.135797: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32 and shape [1003854]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_dataset.take(1):\n",
    "    encoder_inputs_shape = inputs[0].shape\n",
    "    decoder_inputs_shape = inputs[1].shape\n",
    "    targets_shape = targets.shape\n",
    "\n",
    "    print(f'encoder_inputs_shape: {encoder_inputs_shape}')\n",
    "    print(f'decoder_inputs_shape: {decoder_inputs_shape}')\n",
    "    print(f\"targets_shape: {targets_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "210dcfef-d134-4966-bf9b-c7da0f726d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.head_size = n_embd // n_head\n",
    "\n",
    "\n",
    "        # Projecting input into key, query, and value for all attention heads\n",
    "        self.c_attn = layers.Dense(3 * config.n_embd, use_bias=False)\n",
    "        \n",
    "        # Regularization\n",
    "        self.attn_dropout = layers.Dropout(dropout)\n",
    "        self.resid_dropout = layers.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Linear transformation for queries, keys, and values, note that C = n_embd\n",
    "        qkv = self.c_attn(x)  # Input shape: (B, T, C), Output shape: (B, T, 3 * n_embd)\n",
    "\n",
    "        # Split the queries, keys, and values\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)  # Input shape: (B, T, 4 * n_embd), Output shapes: 3 * (B, T, n_embd)\n",
    "        \n",
    "        \n",
    "        # Reshape queries, keys, and values for multi-head attention with head_size = n_embd // num_heads\n",
    "        # BUG: possible issue with tensorflow, you can use tf.reshape(q, (B, T, self.num_heads, -1)), for tensorflow B is unknown: it will give an error\n",
    "        q = tf.reshape(q, (-1, T, n_head, self.head_size))  # Output shape: (B, T, num_heads, head_size)\n",
    "        k = tf.reshape(k, (-1, T, n_head, self.head_size))  # Output shape: (B, T, num_heads, head_size)\n",
    "        v = tf.reshape(v, (-1, T, n_head, self.head_size))  # Output shape: (B, T, num_heads, head_size)\n",
    "\n",
    "\n",
    "\n",
    "        # Transpose queries, keys, and values for efficient matrix multiplication\n",
    "        q = tf.transpose(q, perm=[0, 2, 1, 3])  # Output shape: (B, num_heads, T, head_size)\n",
    "        k = tf.transpose(k, perm=[0, 2, 1, 3])  # Output shape: (B, num_heads, T, head_size)\n",
    "        v = tf.transpose(v, perm=[0, 2, 1, 3])  # Output shape: (B, num_heads, T, head_size)\n",
    "\n",
    "\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = tf.matmul(q, k, transpose_b=True) * (self.head_size ** -0.5)  # Output shape: (B, num_heads, T, T)\n",
    "\n",
    "\n",
    "        mask = tf.linalg.band_part(tf.ones_like(wei), -1, 0)  # Lower triangular matrix of ones\n",
    "        wei = tf.where(mask == 1, wei, float(\"-inf\"))  # Set upper triangular part to -inf\n",
    "\n",
    "\n",
    "        wei = tf.nn.softmax(wei, axis=-1)  # Output shape: (B, num_heads, T, T)\n",
    "        wei = self.attn_dropout(wei)  # Regularization step 1\n",
    "\n",
    "\n",
    "        # Perform the weighted aggregation of the values\n",
    "        out = tf.matmul(wei, v)  # Output shape: (B, num_heads, T, head_size)\n",
    "\n",
    "\n",
    "        # Transpose and reshape the output to match the original shape\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])  # Output shape: (B, T, num_heads, head_size)\n",
    "        out = tf.reshape(out, (-1, T, C))  # Output shape: (B, T, C) - note that C = num_heads * head_size = n_embd\n",
    "        out = self.resid_dropout(out)  # Regularization step 2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6641da18-eeb3-47c6-b4a6-855f5a494b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(4 * n_embd, activation=tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(n_embd),\n",
    "            tf.keras.layers.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        # Apply the feedforward operation and add it to the input\n",
    "        out = x + self.seq(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "96f33997-b4df-461a-ad5a-cdf3f88f7154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(layers.Layer):\n",
    "    def __init__(self, n_embd):\n",
    "        super(Block, self).__init__()\n",
    "       \n",
    "        # Create layers for Multi-Head Attention and FeedForward\n",
    "        self.sa = MultiHeadAttention()\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        # Layer normalization for both branches\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization(epsilon=learning_rate)\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization(epsilon=learning_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Input data is layer normalized: Layer normalizing the input data as the number of features increases over time\n",
    "        x_normalized = self.ln1(x)\n",
    "\n",
    "        # Fed through the attention network: We get the attention scores or weighted values\n",
    "        attn_output = self.sa(x_normalized)\n",
    "\n",
    "        # Added to the input\n",
    "        x = x + attn_output\n",
    "\n",
    "        # Layer normalized the data\n",
    "        x_normalized = self.ln2(x)\n",
    "\n",
    "        # Pass through a FeedForward\n",
    "        ffwd_output = self.ffwd(x_normalized)\n",
    "\n",
    "        # Added to the input\n",
    "        x = x + ffwd_output\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ee0154c-9e00-456c-9e42-d063021dc85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decoder():\n",
    "    \"\"\"\n",
    "    Creates an decoder model based on the provided configuration.\n",
    "    Args:config: An object specifying the configuration parameters.\n",
    "    Returns:decoder: A Keras Model object representing the encoder model.\n",
    "    \"\"\"\n",
    "    # create a dict with all the layers we need\n",
    "    transformer_dict = {\n",
    "        # input layer\n",
    "        'input': tf.keras.Input(shape=(block_size,)),\n",
    "        # word token embeddings\n",
    "        'wte': tf.keras.layers.Embedding(vocab_size, n_embd, input_length=block_size),\n",
    "        # word position embeddings\n",
    "        'wpe': tf.keras.layers.Embedding(block_size, n_embd),\n",
    "        # dropout layer\n",
    "        'drop': tf.keras.layers.Dropout(dropout),\n",
    "        # Transformer blocks\n",
    "        'h': tf.keras.Sequential([Block(n_embd) for _ in range(n_layer)]),\n",
    "        # layer normalization\n",
    "        'ln_f': tf.keras.layers.LayerNormalization(epsilon=learning_rate),\n",
    "        # layer used to project the output of the GPT model to the vocabulary size\n",
    "        'lm_head': tf.keras.layers.Dense(vocab_size, use_bias=False)\n",
    "    }\n",
    "    # input\n",
    "    idx = transformer_dict['input']\n",
    "    pos = tf.range(0, block_size, dtype=tf.int64)  # shape (t)\n",
    "\n",
    "    # Forward the GPT model itself\n",
    "    tok_emb = transformer_dict['wte'](idx)  # token embeddings of shape (b, t, n_embd)\n",
    "    pos_emb = transformer_dict['wpe'](pos)  # position embeddings of shape (t, n_embd)\n",
    "    x = transformer_dict['drop'](tok_emb + pos_emb)\n",
    "    for block in transformer_dict['h'].layers:\n",
    "        x = block(x)\n",
    "    x = transformer_dict['ln_f'](x)\n",
    "\n",
    "    # logit scores for each vocabulary word at each position in the input sequence.\n",
    "    logits = transformer_dict['lm_head'](x)  # shape (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "    # Create encoder model\n",
    "    model = tf.keras.Model(inputs=idx, outputs=logits, name='encoder')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27592e6e-ba83-4143-8881-c98d7ad8b286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 12:39:05.936507: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32 and shape [1003854]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-11-07 12:39:05.936971: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32 and shape [1003854]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  31369/Unknown - 12588s 400ms/step - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 16:08:54.225847: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32 and shape [111540]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-11-07 16:08:54.227491: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32 and shape [111540]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31369/31369 [==============================] - 13100s 417ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/2\n",
      "31369/31369 [==============================] - 12812s 408ms/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "with tf.device(device):    \n",
    "    # Create the decoder model\n",
    "    decoder_model = decoder()\n",
    "\n",
    "\n",
    "    # Compile and train the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    epochs = 2\n",
    "\n",
    "\n",
    "    decoder_model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    history = decoder_model.fit(train_dataset, epochs=epochs, validation_data=test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
